{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无监督学习：降维技术\n",
    "\n",
    "哲学里面讲究：“透过现象看本质”，那么针对机器学习来说，我们观测到的样本，比如图片，往往是抽象的，我们很难就简单的描述把画面中的目标描述清楚。但其实上人在认识这些目标的时候，一定抓住了一些基础特征。如下图：虽然下面的三张图片不完全一样，但他们可以归纳到一个简单的表达上。\n",
    "\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCE9e4995be05030e7b3714f721fb056cb9/19130)\n",
    "\n",
    "这一部分我们就主要来介绍机器学中一些化繁为简的技术：降维。降维的本质就是把一个高维的向量转化为低维的向量。因为高维空间往往比较复杂，因素太多，如果要在高维空间上训练机器学习模型，那势必模型会比较复杂，训练起来困难也会比较大。如果我们能把这些这些高维向量进行降维，去掉其中的冗余的信息，留下本质信息，那么对于我们统计分析就会更友好，也就可以用更简单的机器学习模型去解决问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 聚类\n",
    "\n",
    "我们要介绍的第一种降维的技术就**聚类**，本质上聚类并不是一种降维技术。但聚类却是一种可以很好的对特征进行降维的方式。主要的思想就是：把整个数据的向量按相似度分为K类，每个特征只属于K类中的一类，这样我们就可以每个类的类中心来代表这个特征。每个特征只需要一个记住它属于哪一个类别就可以了。\n",
    "\n",
    "PQ量化里短特征的计算就属于这种方法。\n",
    "\n",
    "### 1.1 k-means\n",
    "\n",
    "最流程一种聚类方法就是K均值聚类，它的算法流程非常简单，也非常直觉同，但问题是K-means必须先确认我们要聚类的的数目，这在很多情况下都不太可能。后续也提过很多改进的方法。\n",
    "\n",
    "1. 我们要把集合$X=\\{x^1,x\\cdots,x^n,\\cdots,x^N\\}$聚成K类\n",
    "2. 初始化每个聚类的类中心$c^i,i=1,2,\\cdots,K$, 可以直接从$X$中随机的挑选。\n",
    "3. 重复下面的步骤：\n",
    "    - 对于$X$中的每个向量$x^n$，计算它与每个聚类中心$c^i$的距离，并找到最接近的那个$c^k$，则将向量归到第$k$类的集合$S_k$中。\n",
    "    - 更新所有的类中心：\n",
    "    $$c^i=\\frac{\\sum_{x^j\\in S^i}x^j}{Num(S^i)}$$\n",
    "    \n",
    "上面步骤中第2步特别关键，如果完全随机初始化，会导致有一些类集合$S_i$是空的，那么后续在计算$Num(S_i)$就等于零了，会出现数值问题。所以这些初始化向量最好直接从原集合中挑选。\n",
    "\n",
    "### 1.2 层次聚类\n",
    "\n",
    "层次聚类（Hierrarchical Agglomerative Clustering, HAC）是一种借助树的结构，进行分层进行聚类的方法，具体的做法：以阈值p，反复进行迭代，每一次迭代法都把上一次迭代结果集合中相似度最大的样本聚合在一起，并用它们的Average代表它们形成一个新的样本作为这次迭代法的输出。这轮迭代没有挑选中的(N-2)也作为下一轮迭代的输出。\n",
    "\n",
    "![](https://img-blog.csdn.net/20180301171047257?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvS2F0aGVyaW5lX2hzcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70))\n",
    "\n",
    "更多的聚类算法可以参考：[常见的六大聚类算法](https://blog.csdn.net/Katherine_hsr/article/details/79382249)\n",
    "\n",
    "### 1.3 分布表达（Distributed Represent）\n",
    "\n",
    "我们开始说，我们可以用每个样本所属的聚类中心$k$来表示这个向量，也就是表示为$[0,0,\\cdots,k,\\cdots 0]$，实际，另一种做法是不那么二值的表示，而是用向量离每个类中心的距离来表示这个向量：$[0.001, 0.001, 0.12, 0.0008, \\cdots, 0.85, 0.025,\\cdots,0.002]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 主成份分析PCA\n",
    "\n",
    "我们来把降维问题转换为一个数学表达，那我们的输入是高维的向量$x$，我们最终是想通过降维的方法，通过这个方法，$x$被降维得到$z$, $z$的维度比$x$要低。\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCEd9a5485d7a3035988401cf2975a72a7a/19133)\n",
    "\n",
    "所以我们最终是希望找到一个这样的变换函数$f$，使得$z=f(x)$。\n",
    "\n",
    "PCA方法中，我们是直接找一个线性变换矩阵$W$，使得$z=W\\cdot x$，那我们如何来选择这个$W$呢。我们的目标是希望降维后的向量，虽然维度低了，但是却有着很强的表达能力，基本上可以代表原来的样本，在原来高维空间中能区分的事物，在降维后的空间中依然能区别。说白了，降维虽然是把特征的维度压缩了，但实际上压缩的是那些对区分目标来说，冗余的信息。\n",
    "\n",
    "利用线性代数的知识，很容易知道，对于$z=W\\cdot x$这个变换来说，这个变换的本质是求$x$在矩阵$W$的列向量空间上的线性组合，即$z$是$x$在$W$列空间上的投影。\n",
    "\n",
    "回来问题上来，如何求$W$呢，我们就为集合$X$找一组正交向量，使得$X$中的每个样本$x$在这组正交向量上投影后，在每一维投影出来的值分布越分散越好，也就是方差越大越好。\n",
    "\n",
    "PCA具体的推导方法就是：先找每一个正交基$w_1$，计算$X$中的每个样本在$w_1$上的投影$z_1$，然后计算所有$z_1$的方差。\n",
    "$$\\sigma(z_1) = \\frac{1}{N}\\sum_{z_1^i}^N(z_1^i-\\bar{z_1})^2$$\n",
    "通过使得$\\sigma(z_1)$最大，求出$w_1$。通过一般列的推导，还要用到Lagrange Multiplier，最后可以推出，$w_1$是$X$的协方差距阵$S＝Cov(x)$的最大的特征值对应的特征向量。\n",
    "\n",
    "然后我们再求$w_2$，要求$w_2$与$w_1$正交，经过推导，我们得到$w_2$是$X$的协方差距阵$S＝Cov(x)$的第二大的特征值对应的特征向量.然后，我们继续找$w_3, \\cdots, w_K$\n",
    "\n",
    "### 2.1 利用PCA来做数据预处理\n",
    "\n",
    "我们可以根据PCA的特性：降维后的特征是原特征在一组正交基下的投影，所以降维后的数据往往各维度之间的相关性都很低，而且分布的中心都是0点。\n",
    "\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCEcb88bcc16c3150cb1adaf0a23c2cddd2/19135)\n",
    "\n",
    "### 2.2 求解PCA的另一种方法\n",
    "\n",
    "上面的内容里，我们求$W$的方法是，一个一个的来求一组正交基$w_i$，我们还可以换个思路来求解$W$。我们希望降维后的$z$能够携带尽可能多的原样本的$x$的信息。那我们考虑用$z$来把$x$恢复出来，如果恢复的越好，那说明这个降维的方法越好，虽然特征维度降了，但信息没丢失。\n",
    "\n",
    "这时候我们的目标就转换为了，优化我们的重建损失：\n",
    "$$x =\\approx \\hat{x}=z_1w_1+z_2w_2+\\cdots+z_kw_k$$\n",
    "$$L=\\min_{z_1,\\cdots,z^K}\\sum\\left \\|x-\\hat{x}\\right \\|=\\min_{z_1,\\cdots,z^K}\\sum\\left \\|x-\\sum_i^Kz_iw_i\\right \\|_2$$\n",
    "\n",
    "找到一组$w_1,\\cdots, w_K$能够让我们重构损失$L$越小越好。\n",
    "\n",
    "形式可写为$X\\approx W\\cdot Z$，其中$W$的列向量是线性无关的基向量。求解方法可以使用SVD分解的方法。\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCE7f55ebd141be23676b82a014b2edb5ec/19137)\n",
    "\n",
    "### 2.3 PCA的局限性\n",
    "\n",
    "PCA是一种线性的方法，其中$x=W\\cdots z$是一种线性变换，所以如果我们要做分类问题，在高维空间上线性不可分的两类目标，在做特征降维后，在低维空间上依然是线性不可分的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 流形学习\n",
    "\n",
    "### 3.1 Locally Linear Embedding(LLE)\n",
    "\n",
    "特征空间中特征的相似性是和特征空间的分布紧密结合在一起的，比如在下面的流形分布上的点，就不再适用欧式距离。\n",
    "\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCE1305dab6a9dea1e917c36ce1471917be/19143)\n",
    "\n",
    "但我们可以发现，如果定义一个很小的局部区域，那么一般的距离公式都还是成立的。有点类似任意连续的函数的一个局部邻域内都可以看成线性的一样。\n",
    "\n",
    "对于降维，我们上面更多的是从保留原特征向量中的信息这个角度去思考的。我们也可以通过分析原样本空间中，样本与样本之间的关系这个角度来思考。我们希望在原空间中距离较近的样本，经过降维后依然比较近。\n",
    "\n",
    "在原空间中，我们先利用插值的方式，对于每一个$x^i$，都找其邻域的一组$x_j$，求得$w_{ij}$，使得下面的值越小越小。\n",
    "$$\\left\\|x^i-\\sum_jw_{ij}x^j\\right\\|_2$$\n",
    "\n",
    "然后我们在降维后的空间，找到$z_i$和$z_j$使得它们利用$w_{ij}$能得到同样的效果。\n",
    "\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCEb490605280261e93e72bb71ab5b5c802/19145)\n",
    "\n",
    "### 3.2 t-SNE\n",
    "\n",
    "Excellent tutorial: https://github.com/oreillymedia/t-SNE-tutorial\n",
    "\n",
    "`t-SNE`方式重新定义了样本与样本之间的关系，它不是用邻域权重来约束，而是用每个点的概率分布来约束。\n",
    "\n",
    "基于距离度量函数$S(x^i, x^j)$，对$X$中的任意一对样本$x_i$和$(x_j)$\n",
    "\n",
    "$$P(x^j|x^i) = \\frac{S(x^i, x^j)}{\\sum_{k\\neq i}S(x^i, x^k)}$$\n",
    "\n",
    "同样，对于降维后的集合$Z$当中的任意两个样本$z_i$和$z_j$，以及降维空间上的距离函数S'\n",
    "$$Q(z^j|z^i) = \\frac{S'(z^i, z^j)}{\\sum_{k\\neq i}S.(z^i, z^k)}$$\n",
    "\n",
    "我们的目标是找到这个一个集合$Z$，满足使得两个分布的KL-Divergence\n",
    "\n",
    "$$L=\\sum_iKL(P(*|x^i)||Q(*|z^i)) = \\sum_i\\sum_jP(x^j|x^i)\\log\\frac{P(x^j|x^i)}{Q(z^j|z^i)}$$\n",
    "\n",
    "在`t-SNE`中：\n",
    "$$S(x^i,x^j) = \\exp\\left(-\\|x^i-x^j\\|_2\\right)$$\n",
    "$$S'(z^i,z^j) = \\frac{1}{1+\\|z^i-z^j\\|_2}$$\n",
    "\n",
    "在原`SNE`方法中，$S‘(z^i,z^j) = \\exp\\left(-\\|z^i-z^j\\|_2\\right)$，只所以`t-SNE`改成了上面这种形式，主要是因为上面形式，对于原空间距离较远的点降维后距离拉得更开一些。\n",
    "\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCE6f3a8561227589bffb5f69c5026433cb/19148)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: A Comarative Review\n",
    "\n",
    "\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCE8f1a11960cf1e5b535bcb74bc21fba59/19141)\n",
    "![](https://note.youdao.com/yws/public/resource/c78ad83387bcb7adf414b412754f03ad/xmlnote/WEBRESOURCEa95ba14cddf68625c45c29ab1badd9e5/19139)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
