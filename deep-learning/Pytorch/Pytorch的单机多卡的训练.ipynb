{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch的单机多卡的训练\n",
    "\n",
    "原始地址：https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个教程主要介绍的是如何利用多GPU卡来进行模型的训练，主要使用到的模块为`DataParallel`\n",
    "\n",
    "在Pytorch中如果想把数据和模型拷贝到其他GPU上是非常容易的：\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda:1\")\n",
    "model.to(device)\n",
    "# 对于Tensor来说，返回的是在device上的新的tensor，而不是对原来tensor直接修改\n",
    "mytensor = my_tensor.to(device)\n",
    "```\n",
    "\n",
    "如果我们希望在多块GPU上`forward/backward`我们的model,那我们就需要使用`DataParallel`了。\n",
    "\n",
    "```python\n",
    "model = nn.DataParallel(model)\n",
    "```\n",
    "\n",
    "下面开始我们的教程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入包和配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:46:00.215892Z",
     "start_time": "2019-06-19T07:45:59.826690Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用GPU设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:46:00.595268Z",
     "start_time": "2019-06-19T07:46:00.217842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPUs on the Node!\n"
     ]
    }
   ],
   "source": [
    "print('There are %d GPUs on the Node!' % (torch.cuda.device_count()))\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作一个随机数填充的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:46:00.610241Z",
     "start_time": "2019-06-19T07:46:00.597160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        # randn　标准正态分布的随机数引擎\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造模型\n",
    "\n",
    "这里为了测试，我们只创建了一个只有一层FC的网络，这不影响对`DateParallel`的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:46:00.629964Z",
     "start_time": "2019-06-19T07:46:00.611851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(\"\\tIn Model: input size\", input.size(), \"output size\",\n",
    "              output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用`DataParallel`把模型分布式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:46:05.715447Z",
     "start_time": "2019-06-19T07:46:00.631503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Model(\n",
       "    (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, output_size)\n",
    "# device_ids参数可以指定使用哪些卡\n",
    "# 注意device指定的gpu id 要与device_ids指定的卡的列表中的第一项匹配\n",
    "model = nn.DataParallel(model, device_ids=[0,1])\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T07:46:10.971066Z",
     "start_time": "2019-06-19T07:46:05.717994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIn Model: input size\tIn Model: input size  torch.Size([15, 5])torch.Size([15, 5])  output sizeoutput size  torch.Size([15, 2])torch.Size([15, 2])\n",
      "\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([15, 5]) \tIn Model: input sizeoutput size  torch.Size([15, 5])Outside: input sizetorch.Size([15, 2])  \n",
      "output sizetorch.Size([30, 5])  output_sizetorch.Size([15, 2]) \n",
      "\tIn Model: input size\tIn Model: input sizetorch.Size([30, 2])  \n",
      "torch.Size([15, 5])torch.Size([15, 5])Outside: input size   output sizeoutput sizetorch.Size([30, 5])   torch.Size([15, 2])torch.Size([15, 2])\tIn Model: input size\tIn Model: input sizeoutput_size\n",
      "\n",
      "   torch.Size([5, 5])torch.Size([5, 5])torch.Size([30, 2])  \n",
      "output sizeoutput sizeOutside: input size   torch.Size([5, 2])torch.Size([5, 2])torch.Size([10, 5])\n",
      "\n",
      " output_size torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "for data in rand_loader:\n",
    "    input = data.to(device)\n",
    "    output = model(input)\n",
    "    print(\"Outside: input size\", input.size(), \"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 背后如何工作的\n",
    "\n",
    "`DataParallel`根据卡数，把每一批(batch size)的数据平均等分，然后每一分片在一块GPU卡上运行Forward和Backward，然后再把结果合并在一起。内部使用的是单进程多线程的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用`DataParallel`后模型中的子模块\n",
    "\n",
    "在单GPU下，我们可以通过下面的代码来单独使用Model的子模块\n",
    "\n",
    "```python\n",
    "model = Model(input_size, output_size)\n",
    "out = model.fc(input)\n",
    "```\n",
    "但是在`DataParallel`中，需要修改为如下：\n",
    "```python\n",
    "model = nn.DataParallel(model)\n",
    "out = model.module.fc(input)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
