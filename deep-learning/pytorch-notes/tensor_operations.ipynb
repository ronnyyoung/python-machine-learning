{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amateur-dream",
   "metadata": {},
   "source": [
    "# Tensor的操作\n",
    "\n",
    "Pytorch中的Tensor大约支持100种以上的操作，其中包括了数学运算、线性代数、矩阵操作（转置、索引、切片等），这些操作都可以跑在CPU或GPU上，这也是Pytorch Tensor的强大之处。\n",
    "\n",
    "我们可以通过这个[页面](https://pytorch.org/docs/stable/torch.html)，来对Tensor支持的所有操作做个大概的了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "passing-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-holocaust",
   "metadata": {},
   "source": [
    "# 索引访值\n",
    "\n",
    "我们可以像访问Numpy.ndarray一样，对torch.Tensor进行各种下标索引与范围切片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "canadian-insulin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "取t的第2行的所有元素：tensor([4, 5, 6, 7])\n",
      "取t的最后一列的所有元素：tensor([ 3,  7, 11])\n",
      "取t的第2列到最后一列的所有元素：tensor([[ 2,  3],\n",
      "        [ 6,  7],\n",
      "        [10, 11]])\n",
      "取t的位置(2,3)上的元素：11\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(12).reshape(3,4)\n",
    "print(f't: {t}')\n",
    "print(f'取t的第2行的所有元素：{t[1]}')\n",
    "print(f'取t的最后一列的所有元素：{t[:, -1]}')\n",
    "print(f'取t的第2列到最后一列的所有元素：{t[:, 2:]}')\n",
    "print(f'取t的位置(2,3)上的元素：{t[2, 3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-adoption",
   "metadata": {},
   "source": [
    "**单一元素的Tensor**\n",
    "\n",
    "当我们通过索引访问Tensor的单一元素时，得到的实际是一个`Tensor`类型的对象，它并不是python中的内置数据类型，我们可以通过Tensor的`item()`方法来获取python对象的标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sound-pattern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t[2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chief-thermal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t[2,3].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-marketplace",
   "metadata": {},
   "source": [
    "# 联合\n",
    "\n",
    "## torch.cat\n",
    "\n",
    "```\n",
    "cat(tensors, dim=0) -> Tensor\n",
    "```\n",
    "\n",
    "`torch.cat`将给定义的tensor的序列(tensors)，按给定义的维度上合并起来，这就要求，这些tensor，除了合并的维度，其他的维度必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "every-parade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8282, -1.3961, -1.6330],\n",
       "        [-0.5860, -1.5504,  0.5470],\n",
       "        [-0.4857, -0.4318, -0.0308],\n",
       "        [ 0.1696, -0.7582,  0.8282],\n",
       "        [-0.1592, -0.7631,  2.8051]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(2,3)\n",
    "t2 = torch.randn(3,3)\n",
    "torch.cat([t1, t2], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-luther",
   "metadata": {},
   "source": [
    "## torch.stack\n",
    "\n",
    "`torch.stack`和`torch.cat`接口用法一致，但它并不是在原有的维度上拼接，而是直接扩展一个新的维度。\n",
    "\n",
    "这就要求，序列中的tensor在维度上必须一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "peripheral-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9310, -2.5397,  0.7603],\n",
       "         [ 0.5423,  0.0121,  2.4951]],\n",
       "\n",
       "        [[-0.9212,  1.1312,  0.4553],\n",
       "         [-0.3836, -2.2080, -0.8785]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn(2,3)\n",
    "t2 = torch.randn(2,3)\n",
    "torch.stack([t1, t2], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-segment",
   "metadata": {},
   "source": [
    "## torch.gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-daughter",
   "metadata": {},
   "source": [
    "# 分片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-ordinance",
   "metadata": {},
   "source": [
    "## torch.split\n",
    "```python\n",
    "split(tensor, split_size_or_sections, dim=0)\n",
    "```\n",
    "`split`将tensor按指定的维度，分拆为多个Tensor的元组，拆分的块chunk的大小是splite_size指定的。可能出现不能整分的情况，这时候最后一块大小一般小于splite_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "identical-discipline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).view(5,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subject-progressive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(a, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-hearts",
   "metadata": {},
   "source": [
    "`split_size_or_sections`也可能是一个list(int)，这时候，它的每个元素，代表每个chunk的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "starting-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]),\n",
       " tensor([[2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2, a3 = torch.split(a, (1,3,1))\n",
    "a1,a2,a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specified-messaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分出来的tensor和原tensor是共享存储的\n",
    "a1[0, 0] = 42\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-charger",
   "metadata": {},
   "source": [
    "## torch.chunk\n",
    "\n",
    "```python\n",
    "chunk(input, chunks, dim=0) -> List of Tensors\n",
    "```\n",
    "`chunk`和`split`功能类似，不同在于，chunk的第二的参数，直接指定的是chunk的数量，最后一个chunk的数量可能会少一些。\n",
    "\n",
    "切分出来的这些Tensor和原Tensor都是共享底层存储的，也就是说每个chunk都是原Tensor的一个view。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-sitting",
   "metadata": {},
   "source": [
    "# 变换操作\n",
    "\n",
    "## torch.reshape\n",
    "\n",
    "```python\n",
    "reshape(input, shape) -> Tensor\n",
    "```\n",
    "`reshape`返回一个和原Tensor具有相同数据，相同数量的Tensor，只是shape不一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-botswana",
   "metadata": {},
   "source": [
    "## torch.view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-triple",
   "metadata": {},
   "source": [
    "## torch.view vs. torch.reshape\n",
    "\n",
    "torch.view has existed for a long time. It will return a tensor with the new shape. The returned tensor will share the underling data with the original tensor. See the documentation here.\n",
    "\n",
    "On the other hand, it seems that torch.reshape has been introduced recently in version 0.4. According to the document, this method will\n",
    "\n",
    "> Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\n",
    "\n",
    "It means that torch.reshape may return a copy or a view of the original tensor. You can not count on that to return a view or a copy. According to the developer:\n",
    "\n",
    "> if you need a copy use clone() if you need the same storage use view(). The semantics of reshape() are that it may or may not share the storage and you don't know beforehand.\n",
    "\n",
    "Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor. Also see here about the meaning of contiguous.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-opening",
   "metadata": {},
   "source": [
    "## torch.transpose\n",
    "\n",
    "```python\n",
    "transpose(input, dim0, dim1) -> Tensor\n",
    "```\n",
    "转置input的指定的2个维度，返回的Tensor和原来的Tensor共享存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "toxic-delay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7794, 0.2979, 0.1634, 0.8068],\n",
       "         [0.6452, 0.9279, 0.0745, 0.8439],\n",
       "         [0.9692, 0.1972, 0.3226, 0.2139]],\n",
       "\n",
       "        [[0.3651, 0.6311, 0.7130, 0.4922],\n",
       "         [0.0043, 0.5986, 0.3140, 0.2075],\n",
       "         [0.4185, 0.1512, 0.0444, 0.4115]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "boxed-jaguar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7794, 0.3651],\n",
       "         [0.6452, 0.0043],\n",
       "         [0.9692, 0.4185]],\n",
       "\n",
       "        [[0.2979, 0.6311],\n",
       "         [0.9279, 0.5986],\n",
       "         [0.1972, 0.1512]],\n",
       "\n",
       "        [[0.1634, 0.7130],\n",
       "         [0.0745, 0.3140],\n",
       "         [0.3226, 0.0444]],\n",
       "\n",
       "        [[0.8068, 0.4922],\n",
       "         [0.8439, 0.2075],\n",
       "         [0.2139, 0.4115]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-beach",
   "metadata": {},
   "source": [
    "## torch.permute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-phoenix",
   "metadata": {},
   "source": [
    "## contiguous\n",
    "\n",
    "There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:\n",
    "\n",
    "`narrow()`, `view()`, `expand()` and `transpose()`\n",
    "\n",
    "For example: when you call transpose(), PyTorch doesn't generate a new tensor with a new layout, it just modifies meta information in the Tensor object so that the offset and stride describe the desired new shape. In this example, the transposed tensor and original tensor share the same memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "practical-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "y = torch.transpose(x, 0, 1)\n",
    "x[0, 0] = 42\n",
    "print(y[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-offense",
   "metadata": {},
   "source": [
    "This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word \"contiguous\" is a bit misleading because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\n",
    "\n",
    "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.\n",
    "\n",
    "Normally you don't need to worry about this. You're generally safe to assume everything will work, and wait until you get a RuntimeError: input is not contiguous where PyTorch expects a contiguous tensor to add a call to contiguous()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-motor",
   "metadata": {},
   "source": [
    "# Math Operations\n",
    "\n",
    "## 三角函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-jacket",
   "metadata": {},
   "source": [
    "## 算术运算\n",
    "\n",
    "我们可以直接在Tensor上面执行`+-*/`等运算符，这些运算会对Tensor执行逐元素计算。这就要求操作符操作的Tensor的维度上必须一致，或者符合Broadcasting。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "taken-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 = tensor([[-1.4154,  0.1441,  1.1729],\n",
      "        [ 0.4147, -0.3196,  1.4436]])\n",
      "t2 = tensor([[ 2.5182, -0.7883, -0.8254],\n",
      "        [-0.3024, -2.4285, -0.0451]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn(2,3)\n",
    "t2 = torch.randn(2,3)\n",
    "print(f't1 = {t1}')\n",
    "print(f't2 = {t2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acoustic-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1028, -0.6442,  0.3475],\n",
       "        [ 0.1123, -2.7481,  1.3985]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于t1.add(t2)或torch.add(t1, t2)\n",
    "t1 + t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "listed-hostel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9336,  0.9324,  1.9983],\n",
       "        [ 0.7171,  2.1088,  1.4887]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于t1.sub(t2)或torch.sub(t1, t2)\n",
    "t1 - t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "insured-proportion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.5643, -0.1136, -0.9681],\n",
       "        [-0.1254,  0.7762, -0.0651]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于t1.mul(t2)或torch.mul(t1, t2)\n",
    "t1 * t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "classical-concentrate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.5621,  -0.1828,  -1.4210],\n",
       "        [ -1.3714,   0.1316, -32.0229]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于t1.div(t2)或torch.div(t1, t2)\n",
    "t1 / t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-burst",
   "metadata": {},
   "source": [
    "需要注意的是这里的乘法并不是数学里的矩阵乘法，Tensor的矩阵乘法，使用的是`torch.matmul`或者`@`运算符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "second-control",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.6460,  0.0251],\n",
       "        [ 0.1047,  0.5857]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于t1.matmul(t2)或torch.matmul(t1, t2)\n",
    "t1 @ t2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-winner",
   "metadata": {},
   "source": [
    "## 对数、指数与幂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-recipient",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "electric-clearing",
   "metadata": {},
   "source": [
    "## 数值截断"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-easter",
   "metadata": {},
   "source": [
    "## 其他操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-morrison",
   "metadata": {},
   "source": [
    "# 降维操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syria",
   "metadata": {},
   "source": [
    "## torch.mean\n",
    "\n",
    "```python\n",
    "'''\n",
    "Args:\n",
    "  input (Tensor): the input tensor.\n",
    "  dim (int or tuple of ints): the dimension or dimensions to reduce.\n",
    "  keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
    "'''\n",
    "mean(input, dim, keepdim=False, *, out=None) -> Tensor\n",
    "```\n",
    "\n",
    "对input沿着`dim`的维度求均值，这样的话，指定的那个维度就会被压缩掉，如果指定了`keepdim=True`的话，那个维度会保留，值为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "centered-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8801,  0.3556,  1.3300, -0.8489, -1.8893, -0.7004],\n",
       "        [-1.1341,  0.7043,  0.0767, -0.9126, -0.9413, -0.5077],\n",
       "        [-0.8743, -2.0277,  0.5664,  0.4266,  2.9812,  0.9459],\n",
       "        [ 0.1711, -2.1501, -1.3418, -1.8992,  0.6031, -0.8814],\n",
       "        [ 0.8263,  1.1446, -1.6875,  1.1150, -0.2767, -0.7673]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(5,6)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "demographic-country",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0262, -0.3947, -0.2112, -0.4238,  0.0954, -0.3822])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#按列的方向(dim=0)将整个Tenoor压缩成为1维的\n",
    "torch.mean(t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "important-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1455],\n",
       "        [-0.4524],\n",
       "        [ 0.3364],\n",
       "        [-0.9164],\n",
       "        [ 0.0591]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-october",
   "metadata": {},
   "source": [
    "对于高维Tensor，我们还可以同时对多个维度进行Reduce，求其均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "statistical-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0852,  1.2012,  0.0452,  0.3130],\n",
       "         [-0.8747, -0.2138,  1.4990, -0.9035],\n",
       "         [-0.0255,  1.0016, -1.6872,  1.2495]],\n",
       "\n",
       "        [[ 1.3288,  0.9973, -1.3797,  1.6270],\n",
       "         [ 2.6580, -0.2791,  0.3662,  1.7222],\n",
       "         [ 2.5107,  0.3394,  1.1392,  0.2362]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2,3,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "written-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6523, 0.4968, 0.5955])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等价于reduce第0维，得到一个3x4的Tensor后，再reduce第1维，得到(3,)的Vector\n",
    "torch.mean(t, dim=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "million-drive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6523, 0.4968, 0.5955])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(0).mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-explorer",
   "metadata": {},
   "source": [
    "## torch.sum\n",
    "\n",
    "`torch.sum`是一个和`torch.mean`用法上很像的操作，只是`sum`的reduce op变成了求和，而不是求均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "separated-illustration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.2181, 3.9745, 4.7637])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(t, dim=(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-resource",
   "metadata": {},
   "source": [
    "## torch.max、torch.min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-coverage",
   "metadata": {},
   "source": [
    "# 对比操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-sierra",
   "metadata": {},
   "source": [
    "## torch.eq、torch.ge、torch.le、torch.gt、torch.lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "weekly-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1975, -0.8859, -0.6680],\n",
       "         [ 1.7126,  0.5976,  0.1704],\n",
       "         [-0.7816,  1.4601,  1.1260]]),\n",
       " tensor([[-0.4304,  0.7686,  0.1742],\n",
       "         [ 0.3721, -1.7648, -0.4187],\n",
       "         [-0.2756, -0.2015, -2.1038]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,3)\n",
    "b = torch.randn(3,3)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "continent-moldova",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True,  True,  True],\n",
       "        [False,  True,  True]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ge(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-still",
   "metadata": {},
   "source": [
    "## torch.sort\n",
    "\n",
    "```python\n",
    "sort(input, dim=-1, descending=False, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`sort`对input按给定义的dim进行升序排列，返回排列后的Tensor的同时，也返回一个对应的下标的重排后的Tensor\n",
    "\n",
    "dim的默认值是Tensor的最后一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "potential-morrison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[ 1.1975, -0.6680, -0.8859],\n",
       "        [ 1.7126,  0.5976,  0.1704],\n",
       "        [ 1.4601,  1.1260, -0.7816]]),\n",
       "indices=tensor([[0, 2, 1],\n",
       "        [0, 1, 2],\n",
       "        [1, 2, 0]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sort(a, dim=1, descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-petersburg",
   "metadata": {},
   "source": [
    "## torch.topk\n",
    "\n",
    "```python\n",
    "topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`topk`返回input中指定维度上，最大的k个元素，以及对应的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pending-nickname",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4168, -1.7439, -0.4161, -0.0458,  0.5801])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "practical-review",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([ 0.5801,  0.4168, -0.0458]),\n",
       "indices=tensor([4, 0, 3]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(a, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-groove",
   "metadata": {},
   "source": [
    "## torch.kthvalue\n",
    "\n",
    "```python\n",
    "kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
    "```\n",
    "`kthvalue`计算输出Tensor的指定维度上第`k`小的元素以及下标。如果dim没有指定，则默认为Tensor的最后一维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "august-chamber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8665,  1.0052, -0.3483],\n",
       "        [ 0.3371, -0.3111, -0.8334],\n",
       "        [-0.9578,  1.3914, -1.5777],\n",
       "        [-1.4825,  1.5743, -0.7107]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cardiovascular-ticket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([-1.4825,  1.0052, -0.8334]),\n",
       "indices=tensor([3, 0, 1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.kthvalue(a, 2, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-queensland",
   "metadata": {},
   "source": [
    "# 频谱操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-shelter",
   "metadata": {},
   "source": [
    "# 其他操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-invitation",
   "metadata": {},
   "source": [
    "## 原地操作(in-place)\n",
    "\n",
    "pytorch的Tensor支持了很多原地操作，它们的特点就是在方法末尾以`_`结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "convinced-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 = tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "after plus 2: t1 = tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(2,3)\n",
    "print(f't1 = {t1}')\n",
    "t1.add_(2)\n",
    "print(f'after plus 2: t1 = {t1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-austria",
   "metadata": {},
   "source": [
    "## 转换为其他数据类型\n",
    "\n",
    "我们可以调用`numpy`接口,返回一个numpy.ndarray的对象，可以调用`tolist`接口，返回一个list的对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "clean-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "multiple-adapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回的ndarray还是和t是共享存储的\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unlimited-patio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(2,3).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
