{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prescribed-relation",
   "metadata": {},
   "source": [
    "# Pytorch中的神经网络基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "imperial-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-amplifier",
   "metadata": {},
   "source": [
    "# nn.Module\n",
    "\n",
    "nn.Module是神经网络结构的表示，它可以表示一个层，也可以表示一个结构块，也可以表示一个完整的模型结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-acting",
   "metadata": {},
   "source": [
    "## 自定义一个layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "protected-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReluLayer()\n"
     ]
    }
   ],
   "source": [
    "class ReluLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReluLayer, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return (x > 0) * x\n",
    "relu = ReluLayer()\n",
    "print(relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "conceptual-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2632, 0.6988, -0.0000],\n",
       "        [0.4950, 0.9098, -0.0000]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "relu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "celtic-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFCLayer()\n"
     ]
    }
   ],
   "source": [
    "# 带参数的Layer\n",
    "class MyFCLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(in_dim, out_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(out_dim))\n",
    "    def forward(self, x):\n",
    "        return x.matmul(self.weights.data) + self.bias.data\n",
    "fclayer = MyFCLayer(25, 10)\n",
    "print(fclayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "light-archives",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.2853, -3.7129,  2.6538, -0.9810,  1.2909, -5.7321,  4.7858, -0.5158,\n",
       "         -8.0954,  4.7340]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 25)\n",
    "fclayer(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-gilbert",
   "metadata": {},
   "source": [
    "## 自定义一个Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "weird-potato",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearReluStack(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ReluLayer()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReluLayer()\n",
      "    (4): MyFCLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearReluStack(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearReluStack, self).__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 100),\n",
    "            ReluLayer(),\n",
    "            nn.Linear(100, 100),\n",
    "            ReluLayer(),\n",
    "            MyFCLayer(100, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)\n",
    "linear_relu_stack = LinearReluStack()\n",
    "print(linear_relu_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "systematic-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0247, -0.1692, -2.0172,  3.7668, -1.2313, -1.1466, -0.7156, -2.9107,\n",
       "         -1.1382,  0.1247]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1,28*28)\n",
    "linear_relu_stack(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-redhead",
   "metadata": {},
   "source": [
    "## 自定义一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "attended-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (stack): LinearReluStack(\n",
      "    (stack): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "      (1): ReluLayer()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (3): ReluLayer()\n",
      "      (4): MyFCLayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = LinearReluStack()\n",
    "    def forward(self, x):\n",
    "        return self.stack(self.flatten(x))\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "official-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2746,  0.5344, -2.2537, -0.1526, -2.7846,  1.7382, -1.8701,  0.2607,\n",
       "          1.9809,  0.8051]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1,28,28)\n",
    "model(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-warner",
   "metadata": {},
   "source": [
    "## 模仿nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "designing-strand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=25, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "    def forward(self, x):\n",
    "        # _modules是内部的一个OrderedDict\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "mlp = MySequential(nn.Linear(25, 100),nn.ReLU(), nn.Linear(100, 10))\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-scout",
   "metadata": {},
   "source": [
    "# 参数\n",
    "\n",
    "每一层的参数，我们可以通过`layer.bias`和`layer.weight`来访问，得到的是一个`nn.parameter.Parameter`的类型对象。\n",
    "\n",
    "对于Sequential的模型，我们可以通过下标来访问每一层：`seqmodel[i]`\n",
    "\n",
    "我们也可以通过`state_dict`来获取nn.Module中的所有层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "competent-pontiac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = nn.Sequential(nn.Linear(25, 100), nn.ReLU(), nn.Linear(100,10))\n",
    "first_layer = mlp[0]\n",
    "first_layer.bias\n",
    "first_layer.weight\n",
    "first_layer.state_dict()\n",
    "type(mlp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ultimate-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', torch.Size([100, 25])) ('0.bias', torch.Size([100])) ('2.weight', torch.Size([10, 100])) ('2.bias', torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "# 获取所有参数\n",
    "print(*[(name, param.shape) for name, param in mlp.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "confirmed-screen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问OrderedDict\n",
    "mlp.state_dict()['2.weight'].data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-pastor",
   "metadata": {},
   "source": [
    "对于`nn.parameter.Parameter`类型的对象，我们可以通过`.data`与`.grad`拿到其数据与梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "international-capacity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_layer.bias.data.shape, first_layer.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-great",
   "metadata": {},
   "source": [
    "# 参数初始化\n",
    "\n",
    "对整个网络应用某个初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "middle-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "mlp.apply(norm_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-operations",
   "metadata": {},
   "source": [
    "单独的某层layer应用初始化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "stuck-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=100, out_features=10, bias=True)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xiaver_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        \n",
    "mlp[2].apply(xiaver_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-printing",
   "metadata": {},
   "source": [
    "# 多个layer共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "complete-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared = nn.Linear(8, 8) #需要共享参数的layer\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared,\n",
    "                    nn.ReLU(), nn.Linear(8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-block",
   "metadata": {},
   "source": [
    "net[2]和net[4]是共享参数的，梯度累加。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
